# Production training config for Cambia Deep CFR
# Derived from benchmark data (2026-02-17, updated 2026-02-18):
#   - Go backend: 180 trav/s (1.28x throughput vs Python)
#   - GPU optimal batch: 4096 (6.7x fwd / 8.5x bwd speedup vs CPU)
#   - Worker scaling: sequential (1 worker) is 2-3x faster than parallel for Go backend
#   - Per-iteration: ~9.5s sequential, ~8.4s pipelined
#   - 750 iterations ≈ 1.75-2 hours
#   - Bottleneck: network backward pass (resolved by columnar buffer)
#
# Hardware: Ryzen 9 5950X (32 cores), 31GB RAM, RTX 3090 24GB

system:
  recursion_limit: 10000

cfr_training:
  num_iterations: 750   # ~9.5s/iter sequential, ~8.4s pipelined × 750 ≈ 1.75-2 hours
  save_interval: 1
  pruning_enabled: true
  pruning_threshold: 1.0e-6
  exploitability_interval: 100
  exploitability_interval_seconds: 7200
  num_workers: 1  # Sequential is fastest for Go backend (IPC overhead dominates)

cfr_plus_params:
  weighted_averaging_enabled: true
  averaging_delay: 100

agent_params:
  memory_level: 1
  time_decay_turns: 3

cambia_rules:
  allowDrawFromDiscardPile: false
  allowReplaceAbilities: false
  snapRace: false
  penaltyDrawCount: 2
  use_jokers: 2
  cards_per_player: 4
  initial_view_count: 2
  cambia_allowed_round: 0
  allowOpponentSnapping: false
  max_game_turns: 46

persistence:
  agent_data_save_path: "strategy/deep_cfr_checkpoint.pt"

agents:
  greedy_agent:
    cambia_call_threshold: 5

analysis:
  exploitability_num_workers: 0

deep_cfr:
  hidden_dim: 256
  dropout: 0.1
  learning_rate: 0.001
  batch_size: 4096       # Optimal GPU throughput (6.7x fwd / 8.5x bwd speedup vs CPU)
  train_steps_per_iteration: 1000  # Calibrated: 1000 steps = 32 epochs/iter, ~43s training/iter
  alpha: 1.5
  traversals_per_step: 1000
  advantage_buffer_capacity: 2000000
  strategy_buffer_capacity: 2000000
  save_interval: 25      # Checkpoint every ~24 min (30 checkpoints total)
  use_gpu: true           # RTX 3090, 2.5-3x speedup over CPU
  sampling_method: "outcome"
  exploration_epsilon: 0.6
  engine_backend: "go"    # Go FFI backend (1.28x throughput vs Python; 180 trav/s)
  pipeline_training: true  # 6-12% wall-clock savings
  use_amp: false           # AMP is a NEGATIVE result for 175K-param net (14-81% slower; Tensor Cores need layers >= 1024)
  use_compile: false       # torch.compile adds 30-90s warmup; enable for long runs on CUDA
  es_validation_interval: 25   # Validate every ~24 min (30 validation points)
  es_validation_depth: 10
  es_validation_traversals: 500  # Reduced from 1000 for faster validation

logging:
  log_level_file: "INFO"
  log_level_console: "WARNING"
  log_dir: "logs"
  log_file_prefix: "cambia"
  log_max_bytes: "10MB"
  log_backup_count: 5
  log_size_update_interval_sec: 60
  worker_config:
    default_level: "WARNING"
    sequential_rules: []
    overrides: []
  log_archive_enabled: true
  log_archive_max_archives: 0
  log_archive_dir: ""
  log_simulation_traces: false
  simulation_trace_filename_prefix: "simulation_traces"
